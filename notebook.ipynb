{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! pip3 install matplotlib plotly scikit-learn xgboost kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import sklearn\n",
    "from sklearn import model_selection, metrics, neighbors, linear_model\n",
    "import xgboost\n",
    "from xgboost import *\n",
    "import os\n",
    "from sklearn import calibration\n",
    "import kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"images\"):\n",
    "    os.mkdir(\"images\")\n",
    "\n",
    "# loading data from csv into dataframe\n",
    "data = pd.read_csv(\"credit_risk_dataset.csv\")\n",
    "\n",
    "# Finding null/nan values in dataframe columns\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# removing null values from dataframe, as they reperesent a small percentage of the 32,000 borrowers\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "# Confirming the null values were removed\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Describing the data set, and identification of outliers:\n",
    "print(data.describe())\n",
    "\n",
    "# Outliers that need to be removed, as these could negatively skew our model\n",
    "\n",
    "# Another way in which we can identify outliers is through various scatterplots.\n",
    "\n",
    "fig = px.scatter_matrix(data,\n",
    "                        dimensions=[\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\"],\n",
    "                        labels={col: col.replace('_', ' ') for col in data.columns},\n",
    "                        height=900, color=\"loan_status\", color_continuous_scale=px.colors.diverging.Tealrose)\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "# One can then observe that income also has an outlier. We remove these as follows\n",
    "data = data[data[\"person_age\"] <= 100]\n",
    "data = data[data[\"person_emp_length\"] <= 100]\n",
    "data = data[data[\"person_income\"] <= 4000000]\n",
    "\n",
    "fig = px.scatter_matrix(data,\n",
    "                        dimensions=[\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\"],\n",
    "                        labels={col: col.replace('_', ' ') for col in data.columns},\n",
    "                        height=900, color=\"loan_status\", color_continuous_scale=px.colors.diverging.Tealrose)\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given the nature of our dataset, we’d expect that we’re dealing with an imbalanced classification problem, \n",
    "meaning that we have considerably more non-default cases than default cases. \n",
    "Using the code below, we confirm that this is indeed the case with 78.4% of our dataset containing non-default cases.\n",
    "\"\"\"\n",
    "\n",
    "# Calculating Percentage of non-default cases\n",
    "data_0 = data[data[\"loan_status\"] == 0].loan_status.count() / data.loan_status.count()\n",
    "print(data_0)\n",
    "\n",
    "\"\"\"\n",
    "With this in mind, we’ll now further explore how loan status is related to other variables in our dataset.\n",
    "\"\"\"\n",
    "# Seeing how loan status relates to other variables\n",
    "fig = px.box(data, x=\"loan_grade\", y=\"loan_percent_income\", color=\"loan_status\",\n",
    "             color_discrete_sequence=px.colors.qualitative.Dark24,\n",
    "             labels={col: col.replace('_', ' ') for col in data.columns},\n",
    "             category_orders={\"loan_grade\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]})\n",
    "fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1))\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "# fig.write_image(\"images/Box.png\")\n",
    "\"\"\"\n",
    "Two things quickly stand out when we look at this box plot. We can clearly see that those who don’t default have a lower loan to income ratio mean value across all loan grades; \\\n",
    "which doesn’t come as a surprise.We can also see that no borrowers with loan grade G were able to repay their loan!\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Using a parallel category diagram, we can understand how different categorical variables\\\n",
    " in our dataset are related to each other and we can map out these relationships\\\n",
    "  on the basis of loan status.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#Parallel category diagram\n",
    "fig = px.parallel_categories(data, color_continuous_scale=px.colors.sequential.RdBu, color=\"Loan_Status\",\n",
    "dimensions=['Home_Status', 'Loan_Intent', \"loan_Grade\", 'Historical_Default'], labels={col:col.replace('_', ' ') for col in data.columns})\n",
    "fig.show()\n",
    "\"\"\"\n",
    "# Parallel category diagram\n",
    "fig = px.parallel_categories(data, color_continuous_scale=px.colors.sequential.RdBu, color=\"loan_status\",\n",
    "                             dimensions=['person_home_ownership', 'loan_intent', \"loan_grade\",\n",
    "                                         'cb_person_default_on_file'],\n",
    "                             labels={col: col.replace('_', ' ') for col in data.columns})\n",
    "# fig.write_image(\"images/Parallel\")\n",
    "# fig.show()\n",
    "\"\"\"\n",
    "Main takeaways from the above diagram:\n",
    "Our dataset is primarily composed of borrowers who have not defaulted on a loan before;\n",
    "  Loan grades “A” and “B” are the most common grades while “F” and “G” are the least common;\n",
    "Home renters defaulted more often on their loans than those with a mortgage, whereas homeowners defaulted the least;\n",
    "Borrowers took out a loan for home improvement the least and for education the most. Also, defaults were more common for loans that were taken up for covering medical expenses and debt consolidation.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Dealing with categorical variables and their labeling with dummy variables\n",
    "\"\"\"\n",
    "df = pd.get_dummies(data=data,\n",
    "                    columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'])\n",
    "print(df)\n",
    "\n",
    "# Splitting dataset\n",
    "Y = df['loan_status']\n",
    "X = df.drop('loan_status', axis=1)\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(X, Y, random_state=0, test_size=0.2)\n",
    "\n",
    "\n",
    "def model_assess(model, name=\"Default\"):\n",
    "    model.fit(x_train, y_train)\n",
    "    preds = model.predict(x_test)\n",
    "    preds_proba = model.predict_proba(x_test)\n",
    "    print('              ', name, '\\n', metrics.classification_report(y_test, model.predict(x_test)))\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=151)\n",
    "model_assess(knn, name=\"KNN\")\n",
    "\n",
    "lg = linear_model.LogisticRegression(random_state=0)\n",
    "model_assess(lg, name=\"LG\")\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n",
    "model_assess(xgb, 'XGBoost')\n",
    "\n",
    "'''\n",
    "f = open(\"classification_report.txt\",\"w\")\n",
    "f.write(model_assess(knn,name=\"KNN\"))\n",
    "f.write(model_assess(lg,name=\"LG\"))\n",
    "f.write(model_assess(xgb, 'XGBoost'))\n",
    "'''\n",
    "\n",
    "# ROC AUC\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "\n",
    "# KNN\n",
    "preds_proba_knn = knn.predict_proba(x_test)\n",
    "probsknn = preds_proba_knn[:, 1]\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, probsknn)\n",
    "aucknn = metrics.roc_auc_score(y_test, probsknn)\n",
    "plt.plot(fpr, tpr, label=f'KNN,AUC={str(round(aucknn, 3))}')\n",
    "\n",
    "# Logistic Regression\n",
    "preds_proba_lg = lg.predict_proba(x_test)\n",
    "probslg = preds_proba_lg[:, 1]\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, probslg)\n",
    "auclg = metrics.roc_auc_score(y_test, probslg)\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression, AUC = {str(round(auclg, 3))}')\n",
    "\n",
    "# XGBOOST\n",
    "preds_proba_xgb = xgb.predict_proba(x_test)\n",
    "probsxgb = preds_proba_xgb[:, 1]\n",
    "fpr, tpr, thresh, = metrics.roc_curve(y_test, probsxgb)\n",
    "aucxgb = metrics.roc_auc_score(y_test, probslg)\n",
    "plt.plot(fpr, tpr, label=f'XGBoost, AUC = {str(round(aucxgb, 3))}')\n",
    "\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.title(\"ROC curve\")\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.legend()\n",
    "plt.savefig(\"images/ROC\")\n",
    "plt.show()\n",
    "\n",
    "# Reliability plot and Brier Score\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "plt.plot([0, 1], [0, 1], color=\"black\")\n",
    "\n",
    "# KNN\n",
    "knn_y, knn_x = calibration.calibration_curve(y_test, preds_proba_knn[:, 1], n_bins=10, normalize=True)\n",
    "lost_knn = metrics.brier_score_loss(y_test, preds_proba_knn[:, 1])\n",
    "plt.plot(knn_x, knn_y, marker='o', label=f'KNN, Brier Score = {str(round(lost_knn, 3))}')\n",
    "\n",
    "# Logistic Regression\n",
    "lg_y, lg_x = calibration.calibration_curve(y_test, preds_proba_lg[:, 1], n_bins=10, normalize=True)\n",
    "loss_lg = metrics.brier_score_loss(y_test, preds_proba_lg[:, 1])\n",
    "plt.plot(lg_x, lg_y, marker='o', label=f'LG, Brier Score = {str(round(loss_lg, 3))}')\n",
    "\n",
    "# XGBOOST\n",
    "preds_proba_xgb = xgb.predict_proba(x_test)\n",
    "xgb_y, xgb_x = calibration.calibration_curve(y_test, preds_proba_xgb[:, 1], n_bins=10, normalize=True)\n",
    "loss_xgb = metrics.brier_score_loss(y_test, preds_proba_xgb[:, 1])\n",
    "plt.plot(xgb_x, xgb_y, marker=\"o\", label=f'XGBoost, Brier Score ={str(round(loss_xgb, 3))}')\n",
    "\n",
    "plt.ylabel(\"Actual Probability\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.title(\"Reliability Plot\")\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.legend()\n",
    "plt.savefig(\"images/RealiabilityPlot\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importance plot\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(15, 17), ncols=1, nrows=2)\n",
    "plt.subplots_adjust(left=0.125, right=0.9, bottom=0.1, top=0.9, wspace=0, hspace=0.5)\n",
    "plot_importance(xgb, importance_type='gain', ax=ax1)\n",
    "ax1.set_title('Feature Importance by Information Gain', fontsize=18)\n",
    "ax1.set_xlabel('Gain')\n",
    "plt.savefig(\"images/FeatureImportance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
